

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>sktime.regression.compose._ensemble &mdash; sktime 0.4.1 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/fields.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../_static/sktime-favicon.ico"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="author" title="About these documents" href="../../../../about.html" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> sktime
          

          
          </a>

          
            
            
              <div class="version">
                0.4.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../how_to_get_started.html">How to get started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_reference.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../contributing.html">How to contribute</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../about.html">About us</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../governance.html">Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../code_of_conduct.html">Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../contributors.html">Contributors</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">sktime</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>sktime.regression.compose._ensemble</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for sktime.regression.compose._ensemble</h1><div class="highlight"><pre>
<span></span><span class="n">__author__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Markus LÃ¶ning&quot;</span><span class="p">,</span> <span class="s2">&quot;Ayushmaan Seth&quot;</span><span class="p">]</span>
<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;TimeSeriesForestRegressor&quot;</span><span class="p">]</span>

<span class="kn">from</span> <span class="nn">warnings</span> <span class="kn">import</span> <span class="n">warn</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="kn">import</span> <span class="nn">numbers</span>

<span class="kn">from</span> <span class="nn">sklearn.ensemble._base</span> <span class="kn">import</span> <span class="n">_partition_estimators</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sktime.transformers.series_as_features.summarize</span> <span class="kn">import</span> \
    <span class="n">RandomIntervalFeatureExtractor</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble._forest</span> <span class="kn">import</span> <span class="n">_generate_unsampled_indices</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble._forest</span> <span class="kn">import</span> <span class="n">_get_n_samples_bootstrap</span>
<span class="kn">from</span> <span class="nn">sktime.utils.time_series</span> <span class="kn">import</span> <span class="n">time_series_slope</span>
<span class="kn">from</span> <span class="nn">sktime.utils.validation.series_as_features</span> <span class="kn">import</span> <span class="n">check_X</span><span class="p">,</span> <span class="n">check_X_y</span>
<span class="kn">from</span> <span class="nn">sktime.regression.base</span> <span class="kn">import</span> <span class="n">BaseRegressor</span>
<span class="kn">from</span> <span class="nn">sktime.series_as_features.base.estimators._ensemble</span> <span class="kn">import</span> \
    <span class="n">BaseTimeSeriesForest</span>


<div class="viewcode-block" id="TimeSeriesForestRegressor"><a class="viewcode-back" href="../../../../modules/auto_generated/sktime.regression.compose.TimeSeriesForestRegressor.html#sktime.regression.TimeSeriesForestRegressor">[docs]</a><span class="k">class</span> <span class="nc">TimeSeriesForestRegressor</span><span class="p">(</span><span class="n">BaseTimeSeriesForest</span><span class="p">,</span> <span class="n">BaseRegressor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Time-Series Forest Regressor.</span>

<span class="sd">    A time series forest is a meta estimator and an adaptation of the random</span>
<span class="sd">    forest for time-series/panel data that fits a number of decision tree</span>
<span class="sd">    regressors on various sub-samples of a transformed dataset and uses</span>
<span class="sd">    averaging to improve the predictive accuracy and control over-fitting.</span>
<span class="sd">    The sub-sample size is always the same as the original input sample size</span>
<span class="sd">    but the samples are drawn with replacement if `bootstrap=True` (default).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator : Pipeline</span>
<span class="sd">        A pipeline consisting of series-to-tabular transformers</span>
<span class="sd">        and a decision tree regressor as final estimator.</span>
<span class="sd">    n_estimators : integer, optional (default=100)</span>
<span class="sd">        The number of trees in the forest.</span>
<span class="sd">    criterion : string, optional (default=&quot;mse&quot;)</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria</span>
<span class="sd">        are &quot;mse&quot; for the mean squared error, which is equal to variance</span>
<span class="sd">        reduction as feature selection criterion and minimizes the L2 loss</span>
<span class="sd">        using the mean of each terminal node, &quot;friedman_mse&quot;, which uses mean</span>
<span class="sd">        squared error with Friedman&#39;s improvement score for potential splits,</span>
<span class="sd">        and &quot;mae&quot; for the mean absolute error, which minimizes the L1 loss</span>
<span class="sd">        using the median of each terminal node.</span>
<span class="sd">    max_depth : integer or None, optional (default=None)</span>
<span class="sd">        The maximum depth of the tree. If None, then nodes are expanded until</span>
<span class="sd">        all leaves are pure or until all leaves contain less than</span>
<span class="sd">        min_samples_split samples.</span>
<span class="sd">    min_samples_split : int, float, optional (default=2)</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>
<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>
<span class="sd">    min_samples_leaf : int, float, optional (default=1)</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>
<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>
<span class="sd">    min_weight_fraction_leaf : float, optional (default=0.)</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>
<span class="sd">    max_features : int, float, string or None, optional (default=&quot;auto&quot;)</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>
<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a fraction and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;auto&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)` (same as &quot;auto&quot;).</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>
<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>
<span class="sd">    max_leaf_nodes : int or None, optional (default=None)</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>
<span class="sd">    min_impurity_decrease : float, optional (default=0.)</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>
<span class="sd">        The weighted impurity decrease equation is the following::</span>
<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>
<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>
<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>
<span class="sd">    min_impurity_split : float, (default=1e-7)</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>
<span class="sd">    bootstrap : boolean, optional (default=True)</span>
<span class="sd">        Whether bootstrap samples are used when building trees.</span>
<span class="sd">    oob_score : bool (default=False)</span>
<span class="sd">        Whether to use out-of-bag samples to estimate</span>
<span class="sd">        the generalization accuracy.</span>
<span class="sd">    n_jobs : int or None, optional (default=None)</span>
<span class="sd">        The number of jobs to run in parallel for both `fit` and `predict`.</span>
<span class="sd">        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.</span>
<span class="sd">        ``-1`` means using all processors.</span>
<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>
<span class="sd">    verbose : int, optional (default=0)</span>
<span class="sd">        Controls the verbosity when fitting and predicting.</span>
<span class="sd">    warm_start : bool, optional (default=False)</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just fit a whole</span>
<span class="sd">        new forest.</span>
<span class="sd">    None, optional (default=None)</span>
<span class="sd">        Weights associated with classes in the form ``{class_label: weight}``.</span>
<span class="sd">        If not given, all classes are supposed to have weight one. For</span>
<span class="sd">        multi-output problems, a list of dicts can be provided in the same</span>
<span class="sd">        order as the columns of y.</span>
<span class="sd">        Note that for multioutput (including multilabel) weights should be</span>
<span class="sd">        defined for each class of every column in its own dict. For example,</span>
<span class="sd">        for four-class multilabel classification weights should be</span>
<span class="sd">        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of</span>
<span class="sd">        [{1:1}, {2:5}, {3:1}, {4:1}].</span>
<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``</span>
<span class="sd">        The &quot;balanced_subsample&quot; mode is the same as &quot;balanced&quot; except that</span>
<span class="sd">        weights are computed based on the bootstrap sample for every tree</span>
<span class="sd">        grown.</span>
<span class="sd">        For multi-output, the weights of each column of y will be multiplied.</span>
<span class="sd">        Note that these weights will be multiplied with sample_weight (passed</span>
<span class="sd">        through the fit method) if sample_weight is specified.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimators_ : list of DecisionTreeRegressor</span>
<span class="sd">        The collection of fitted sub-estimators.</span>
<span class="sd">    n_features_ : int</span>
<span class="sd">        The number of features when ``fit`` is performed.</span>
<span class="sd">    n_outputs_ : int</span>
<span class="sd">        The number of outputs when ``fit`` is performed.</span>
<span class="sd">    feature_importances_ : array of shape = [n_features]</span>
<span class="sd">        The feature importances (the higher, the more important the feature).</span>
<span class="sd">    oob_score_ : float</span>
<span class="sd">        Score of the training dataset obtained using an out-of-bag estimate.</span>
<span class="sd">    oob_decision_function_ : array of shape = [n_samples, n_classes]</span>
<span class="sd">        Decision function computed with out-of-bag estimate on the training</span>
<span class="sd">        set. If n_estimators is small it might be possible that a data point</span>
<span class="sd">        was never left out during the bootstrap. In this case,</span>
<span class="sd">        `oob_decision_function_` might contain NaN.</span>
<span class="sd">    class_weight: dict, list of dicts, &quot;balanced&quot;, &quot;balanced_subsample&quot; or \</span>
<span class="sd">        None, optional (default=None)</span>
<span class="sd">        Not needed here, added in the constructor to align with base class \</span>
<span class="sd">            sharing both Classifier and Regressor parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="TimeSeriesForestRegressor.__init__"><a class="viewcode-back" href="../../../../modules/auto_generated/sktime.regression.compose.TimeSeriesForestRegressor.html#sktime.regression.TimeSeriesForestRegressor.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">max_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">estimator</span>
        <span class="c1"># Assign values, even though passed on to base estimator below,</span>
        <span class="c1"># necessary here for cloning</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span> <span class="o">=</span> <span class="n">min_impurity_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_samples</span> <span class="o">=</span> <span class="n">max_samples</span>

        <span class="c1"># Pass on params.</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TimeSeriesForestRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">max_samples</span><span class="o">=</span><span class="n">max_samples</span>
        <span class="p">)</span>

        <span class="c1"># We need to add is-fitted state when inheriting from scikit-learn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_fitted</span> <span class="o">=</span> <span class="kc">False</span></div>

    <span class="k">def</span> <span class="nf">_validate_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;n_estimators must be an integer, &quot;</span>
                             <span class="s2">&quot;got </span><span class="si">{0}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">)))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;n_estimators must be greater than zero, &quot;</span>
                             <span class="s2">&quot;got </span><span class="si">{0}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">))</span>

        <span class="c1"># Set base estimator</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Set default time series forest</span>
            <span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">,</span> <span class="n">time_series_slope</span><span class="p">]</span>
            <span class="n">steps</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;transform&#39;</span><span class="p">,</span>
                     <span class="n">RandomIntervalFeatureExtractor</span><span class="p">(</span>
                        <span class="n">n_intervals</span><span class="o">=</span><span class="s1">&#39;sqrt&#39;</span><span class="p">,</span>
                        <span class="n">features</span><span class="o">=</span><span class="n">features</span><span class="p">,</span>
                        <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)),</span>
                     <span class="p">(</span><span class="s1">&#39;clf&#39;</span><span class="p">,</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span>
                        <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">))]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimator_</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># else check given estimator is a pipeline with prior</span>
            <span class="c1"># transformations and final decision tree</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">,</span> <span class="n">Pipeline</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;`estimator` must be &#39;</span>
                                 <span class="s1">&#39;pipeline with transforms.&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                              <span class="n">DecisionTreeRegressor</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Last step in `estimator` must be &#39;</span>
                                 <span class="s1">&#39;DecisionTreeRegressor.&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimator_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span>

        <span class="c1"># Set parameters according to naming in pipeline</span>
        <span class="n">estimator_params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;criterion&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">,</span>
            <span class="s2">&quot;max_depth&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span><span class="p">,</span>
            <span class="s2">&quot;min_samples_split&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span><span class="p">,</span>
            <span class="s2">&quot;min_samples_leaf&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span><span class="p">,</span>
            <span class="s2">&quot;min_weight_fraction_leaf&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
            <span class="s2">&quot;max_features&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span><span class="p">,</span>
            <span class="s2">&quot;max_leaf_nodes&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span><span class="p">,</span>
            <span class="s2">&quot;min_impurity_decrease&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span><span class="p">,</span>
            <span class="s2">&quot;min_impurity_split&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">final_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator_</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimator_params</span> <span class="o">=</span> <span class="p">{</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">final_estimator</span><span class="si">}</span><span class="s1">__</span><span class="si">{</span><span class="n">pname</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">:</span> <span class="n">pval</span>
                                 <span class="k">for</span> <span class="n">pname</span><span class="p">,</span> <span class="n">pval</span>
                                 <span class="ow">in</span> <span class="n">estimator_params</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

        <span class="c1"># Set renamed estimator parameters</span>
        <span class="k">for</span> <span class="n">pname</span><span class="p">,</span> <span class="n">pval</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">pname</span><span class="p">,</span> <span class="n">pval</span><span class="p">)</span>

<div class="viewcode-block" id="TimeSeriesForestRegressor.predict"><a class="viewcode-back" href="../../../../modules/auto_generated/sktime.regression.compose.html#sktime.regression.TimeSeriesForestRegressor.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict regression target for X.</span>
<span class="sd">        The predicted regression target of an input sample is computed as the</span>
<span class="sd">        mean predicted regression targets of the trees in the forest.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix of shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csr_matrix``.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : array of shape = [n_samples] or [n_samples, n_outputs]</span>
<span class="sd">            The predicted values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_is_fitted</span><span class="p">()</span>
        <span class="c1"># Check data</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_X</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">enforce_univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># Assign chunk of trees to jobs</span>
        <span class="n">n_jobs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_partition_estimators</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)</span>

        <span class="c1"># Parallel loop</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">predict</span><span class="p">)(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_set_oob_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute out-of-bag scores.&quot;&quot;&quot;</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">enforce_univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">))</span>
        <span class="n">n_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">))</span>

        <span class="n">n_samples_bootstrap</span> <span class="o">=</span> <span class="n">_get_n_samples_bootstrap</span><span class="p">(</span>
            <span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_samples</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
            <span class="n">final_estimator</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">unsampled_indices</span> <span class="o">=</span> <span class="n">_generate_unsampled_indices</span><span class="p">(</span>
                <span class="n">final_estimator</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_samples_bootstrap</span><span class="p">)</span>
            <span class="n">p_estimator</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
                <span class="n">X</span><span class="p">[</span><span class="n">unsampled_indices</span><span class="p">,</span> <span class="p">:],</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">p_estimator</span> <span class="o">=</span> <span class="n">p_estimator</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

            <span class="n">predictions</span><span class="p">[</span><span class="n">unsampled_indices</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">p_estimator</span>
            <span class="n">n_predictions</span><span class="p">[</span><span class="n">unsampled_indices</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">n_predictions</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Some inputs do not have OOB scores. &quot;</span>
                 <span class="s2">&quot;This probably means too few trees were used &quot;</span>
                 <span class="s2">&quot;to compute any reliable oob estimates.&quot;</span><span class="p">)</span>
            <span class="n">n_predictions</span><span class="p">[</span><span class="n">n_predictions</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">predictions</span> <span class="o">/=</span> <span class="n">n_predictions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">oob_prediction_</span> <span class="o">=</span> <span class="n">predictions</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oob_prediction_</span> <span class="o">=</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">oob_prediction_</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">oob_score_</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oob_score_</span> <span class="o">+=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">[:,</span> <span class="n">k</span><span class="p">],</span>
                                        <span class="n">predictions</span><span class="p">[:,</span> <span class="n">k</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">oob_score_</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span>

    <span class="k">def</span> <span class="nf">_validate_y_class_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># in regression, we don&#39;t validate class weights</span>
        <span class="c1"># TODO remove from regression</span>
        <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="kc">None</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2019 - 2020, sktime developers (BSD-3-Clause License)

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>